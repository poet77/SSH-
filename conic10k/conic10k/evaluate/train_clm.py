import logging
import os
import sys
from dataclasses import dataclass, field
from typing import Optional

import datasets
from data import get_dataset
from args import *
import torch

import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, Trainer, TrainingArguments, set_seed
from transformers.testing_utils import CaptureLogger
from datasets import Dataset

from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model, PrefixTuningConfig

logger = logging.getLogger(__name__)

@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """
    model_name_or_path: str = field(
        default=None,
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    low_cpu_mem_usage: bool = field(
        default=False,
        metadata={
            "help": (
                "It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded."
                "set True will benefit LLM loading time and RAM consumption."
            )
        },
    )
    load_in_8bit: bool = field(default=False)


def main():
    # See all possible arguments in src/transformers/training_args.py
    # or by passing the --help flag to this script.
    # We now keep distinct sets of args, for a cleaner separation of concerns.

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments, PeftArgs))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments.
        model_args, data_args, training_args, lora_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args, lora_args = parser.parse_args_into_dataclasses()

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    if training_args.should_log:
        # The default of training_args.log_level is passive, so we set log level at info here to have that default.
        transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )
    logger.info(f"Training/evaluation parameters {training_args}")

    # Set seed before initializing model.
    set_seed(training_args.seed)

    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, local_files_only=True, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    assert tokenizer.eos_token_id is not None

    raw_datasets = get_dataset(path=data_args.dataset_path, task=data_args.task)

    # Preprocessing the datasets.
    # First we tokenize all the texts.
    if training_args.do_train:
        column_names = list(raw_datasets["train"].features)
    else:
        column_names = list(raw_datasets["validation"].features)
    text_column_name = "text" if "text" in column_names else column_names[0]

    # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function
    tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        outputs = {}
        with CaptureLogger(tok_logger) as cl:
            if data_args.no_instruct_loss:
                input = tokenizer.encode(examples['input'])
                labels = tokenizer.encode(examples['labels'])[1:]

                outputs['input_ids'] = input + labels + [tokenizer.eos_token_id]
                outputs['labels'] = len(input) * [-100] + labels + [tokenizer.eos_token_id]
            else:
                outputs['input_ids'] = tokenizer.encode(examples['input'] + ' '+ examples['labels']) + [tokenizer.eos_token_id]
                outputs['labels'] = outputs['input_ids']

            outputs['attention_mask'] = [1] * len(outputs['input_ids'])

            outputs['input_ids'] = torch.tensor(outputs['input_ids'], dtype=torch.long)
            outputs['attention_mask'] = torch.tensor(outputs['attention_mask'], dtype=torch.long)
            outputs['labels'] = torch.tensor(outputs['labels'], dtype=torch.long)

        # clm input could be much much longer than block_size
        if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return outputs

    with training_args.main_process_first(desc="dataset map tokenization"):
        tokenized_datasets = raw_datasets.map(
            tokenize_function,
            batched=False,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Running tokenizer on dataset",
        )

    if training_args.do_train:
        if "train" not in tokenized_datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = tokenized_datasets["train"]
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))

    if training_args.do_eval:
        if "validation" not in tokenized_datasets:
            raise ValueError("--do_eval requires a validation dataset")
        eval_dataset = tokenized_datasets["validation"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))

    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path,
        torch_dtype=model_args.torch_dtype,
        trust_remote_code=True
    )

    model.resize_token_embeddings(len(tokenizer))

    if lora_args.use_lora:
        peft_config = LoraConfig(
            r=lora_args.lora_rank,
            lora_alpha=lora_args.lora_alpha,
            lora_dropout=0.05,
            task_type="CAUSAL_LM",
        )

        model = get_peft_model(model, peft_config)

    train_samples = []
    for batch in train_dataset:
        # 转换 batch 中的每个键值对
        batch_tensor = {key: torch.tensor(value) if isinstance(value, list) else value for key, value in batch.items()}
        
        # 将每个样本从 batch 中分离并存储到列表
        num_samples = len(next(iter(batch_tensor.values())))  # 样本数量
        for i in range(num_samples):
            sample = {key: value[i].tolist() if isinstance(value, torch.Tensor) else value[i] for key, value in batch_tensor.items()}
            train_samples.append(sample)

    # 创建新的 Dataset 对象
    new_train_dataset = Dataset.from_dict({key: [sample[key] for sample in train_samples] for key in train_samples[0]})

    converted_samples = []
    for batch in eval_dataset:
        # 转换 batch 中的每个键值对
        batch_tensor = {key: torch.tensor(value) if isinstance(value, list) else value for key, value in batch.items()}
        
        # 将每个样本从 batch 中分离并存储到列表
        num_samples = len(next(iter(batch_tensor.values())))  # 样本数量
        for i in range(num_samples):
            sample = {key: value[i].tolist() if isinstance(value, torch.Tensor) else value[i] for key, value in batch_tensor.items()}
            converted_samples.append(sample)

    # 创建新的 Dataset 对象
    new_eval_dataset = Dataset.from_dict({key: [sample[key] for sample in converted_samples] for key in converted_samples[0]})

    # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=new_train_dataset if training_args.do_train else None,
        eval_dataset=new_eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        # Data collator will default to DataCollatorWithPadding, so we change it.
        data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, return_tensors="pt", padding=True, pad_to_multiple_of=8),
        compute_metrics=None,
        preprocess_logits_for_metrics=None
    )

    # Training
    if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
        trainer.save_model()  # Saves the tokenizer too for easy upload


if __name__ == "__main__":
    main()